# Performance and Interpretability Trade-offs in Reinforcement Learning for Sepsis Treatment

[![Paper](https://img.shields.io/badge/Paper-PDF-red)](paper/main.pdf)
[![License](https://img.shields.io/badge/License-Academic-blue)]()
[![Python](https://img.shields.io/badge/Python-3.10-green)]()

**Authors:** Zhiyu Cheng, Yalun Ding, Chuanhui Peng (Equal contribution)
**Course:** STAT 8289 - Reinforcement Learning
**Institution:** George Washington University
**Date:** October 2025

---

## ğŸ¯ Project Overview

This project investigates the **performance-interpretability trade-off** in reinforcement learning for sepsis treatment, challenging two prevailing assumptions:
1. **Interpretability inevitably compromises performance**
2. **Online RL methods necessarily outperform offline approaches**

### Key Findings

- **Performance**: Online RL achieves marginally higher survival (95.4% vs 94.2%), with only 1.9 percentage point advantage on high-severity patients
- **Interpretability**: Offline CQL exhibits **600-fold stronger** feature importance signals compared to DQN (40.06 vs 0.069)
- **Clinical Deployment**: CQL delivers comparable survival rates with transparent decision logic and no patient risk during training

### Novel Contributions

1. **First quantitative interpretability comparison** using Linearly Estimated Gradients (LEG) across offline and online RL algorithms
2. **Demonstration that the performance-interpretability trade-off is not inevitable** - CQL achieves both high performance and superior interpretability
3. **Comprehensive evaluation** of 8 methods (2 baselines, 3 offline RL, 3 online RL) with SOFA-stratified analysis

---

## ğŸ“„ Paper

**Full Paper:** [paper/main.pdf](paper/main.pdf) (29 pages, JASA format)

### Abstract

Sepsis remains a leading cause of mortality in critical care, and reinforcement learning (RL) offers a promising route to data-driven treatment policies. Yet clinical adoption is impeded by the prevailing assumption that interpretability inevitably compromises performance, and that online RL methods necessarily outperform offline approaches. We interrogate these trade-offs by comparing three offline RL methods (Behavior Cloning, Conservative Q-Learning, and Deep Q-Network trained on static datasets) with three online RL methods (Double DQN with Attention, Double DQN with Residual connections, and Soft Actor-Critic with environment interaction) using the gym-sepsis simulator.

Across 500 evaluation episodes, online RL achieves marginally higher overall survival (95.4% for DDQN-Attention vs. 94.2% for BC), with a 1.9 percentage point advantage on high-severity patients (90.5% vs. 88.6%). However, this modest performance gain comes at the cost of requiring extensive environment interaction during trainingâ€”infeasible in clinical settings. Interpretability analysis reveals that offline methods, particularly CQL, produce LEG saliency peaks of 40.06â€”roughly 600-fold larger than DQN's 0.069â€”highlighting clinically coherent emphasis on blood pressure and lactate levels.

**Keywords:** Reinforcement Learning, Sepsis Treatment, Interpretability, Conservative Q-Learning, LEG Analysis, Offline RL, MIMIC-III

---

## ğŸš€ Quick Start

### View Results

```bash
# View the complete paper
open paper/main.pdf

# View key figures
open results/figures/algorithm_comparison.png
open results/figures/leg_interpretability_comparison.png
```

### Reproduce Experiments

```bash
# 1. Setup environment
conda create -n sepsis_rl python=3.10
conda activate sepsis_rl
pip install -r requirements.txt

# 2. Evaluate all models
python scripts/re_evaluate_all.py

# 3. Run LEG interpretability analysis
python scripts/Interpret_LEG/leg_analysis_offline.py
python scripts/Interpret_LEG/leg_analysis_online.py

# 4. Generate figures
python scripts/create_leg_comparison_figure.py
python scripts/06_visualization.py
```

---

## ğŸ“Š Key Results

### Overall Performance (500 Episodes)

| Algorithm | Type | Survival (%) | Avg Return | Avg Length |
|-----------|------|--------------|------------|------------|
| **DDQN-Attention** | Online | **95.4** | 13.62 Â± 6.28 | 7.9 Â± 1.0 |
| SAC | Online | 94.8 | 13.44 Â± 6.66 | 7.7 Â± 1.2 |
| BC | Offline | 94.2 | 13.26 Â± 7.01 | 9.5 Â± 0.6 |
| CQL | Offline | 94.0 | 13.20 Â± 7.12 | 9.5 Â± 0.5 |
| DQN | Offline | 94.0 | 13.20 Â± 7.12 | 7.8 Â± 1.2 |

### High-Severity Patients (SOFA â‰¥ 11)

| Algorithm | Type | Survival (%) | Avg Return |
|-----------|------|--------------|------------|
| **DDQN-Attention** | Online | **90.5** | 12.16 Â± 8.79 |
| SAC | Online | 88.7 | 11.62 Â± 9.49 |
| BC | Offline | 88.6 | 11.63 Â± 9.82 |
| CQL | Offline | 88.5 | 11.55 Â± 9.95 |
| DQN | Offline | 84.3 | 10.29 Â± 11.46 |

### Interpretability (LEG Analysis)

| Algorithm | Max Saliency | Interpretability | Clinical Deployment |
|-----------|--------------|------------------|---------------------|
| **CQL** | **40.06** | Excellent | Suitable |
| BC | 0.78 | Mixed | Requires validation |
| DQN | 0.069 | Poor | Not suitable |

**600-fold difference** between CQL and DQN in maximum saliency magnitude!

---

## ğŸ—‚ï¸ Project Structure

```
project_1/
â”œâ”€â”€ paper/                          # Complete 29-page paper (JASA format)
â”‚   â”œâ”€â”€ main.pdf                    # Final PDF
â”‚   â”œâ”€â”€ main.tex                    # LaTeX source
â”‚   â”œâ”€â”€ sections/                   # Paper sections
â”‚   â”‚   â”œâ”€â”€ 01_introduction.tex
â”‚   â”‚   â”œâ”€â”€ 02_related.tex
â”‚   â”‚   â”œâ”€â”€ 03_problem.tex
â”‚   â”‚   â”œâ”€â”€ 04_methods.tex
â”‚   â”‚   â”œâ”€â”€ 05_results.tex
â”‚   â”‚   â”œâ”€â”€ 06_discussion.tex
â”‚   â”‚   â”œâ”€â”€ 07_conclusion.tex
â”‚   â”‚   â”œâ”€â”€ 08_contributions.tex
â”‚   â”‚   â””â”€â”€ 09_appendix.tex
â”‚   â””â”€â”€ references.bib              # Bibliography (35+ references)
â”‚
â”œâ”€â”€ src/                            # Source code
â”‚   â”œâ”€â”€ envs/                       # Environment wrappers
â”‚   â”‚   â”œâ”€â”€ reward_functions.py    # Reward function implementations
â”‚   â”‚   â””â”€â”€ sepsis_wrapper.py      # Gym-sepsis wrapper
â”‚   â”œâ”€â”€ evaluation/                 # Evaluation utilities
â”‚   â”‚   â””â”€â”€ metrics.py             # Performance metrics
â”‚   â”œâ”€â”€ data/                       # Data collection
â”‚   â”‚   â””â”€â”€ collect_data.py        # Dataset generation
â”‚   â””â”€â”€ visualization/              # Visualization tools
â”‚       â”œâ”€â”€ interpretability.py    # LEG analysis
â”‚       â””â”€â”€ policy_viz.py          # Policy visualization
â”‚
â”œâ”€â”€ scripts/                        # Experiment scripts
â”‚   â”œâ”€â”€ 01_baseline_evaluation.py  # Baseline policies
â”‚   â”œâ”€â”€ 02_train_bc.py             # Behavior Cloning
â”‚   â”œâ”€â”€ 03_train_cql.py            # Conservative Q-Learning
â”‚   â”œâ”€â”€ 04_train_dqn.py            # Deep Q-Network
â”‚   â”œâ”€â”€ 06_visualization.py        # Generate figures
â”‚   â”œâ”€â”€ 07_final_analysis.py       # Final analysis
â”‚   â”œâ”€â”€ evaluate_yalun_models.py   # Online RL evaluation
â”‚   â”œâ”€â”€ re_evaluate_all.py         # Re-evaluate all models
â”‚   â””â”€â”€ Interpret_LEG/             # LEG interpretability analysis
â”‚       â”œâ”€â”€ leg_analysis_offline.py
â”‚       â””â”€â”€ leg_analysis_online.py
â”‚
â”œâ”€â”€ results/                        # All experimental results
â”‚   â”œâ”€â”€ figures/                    # Visualization outputs
â”‚   â”‚   â”œâ”€â”€ algorithm_comparison.png              # Main performance figure
â”‚   â”‚   â”œâ”€â”€ leg_interpretability_comparison.png   # Main interpretability figure
â”‚   â”‚   â””â”€â”€ leg/                   # Detailed LEG analysis (90+ figures)
â”‚   â”œâ”€â”€ models/                     # Trained models
â”‚   â”‚   â”œâ”€â”€ bc_simple_reward.d3
â”‚   â”‚   â””â”€â”€ cql_simple_reward.d3
â”‚   â”œâ”€â”€ baseline_results.pkl        # Baseline evaluation
â”‚   â”œâ”€â”€ bc_results.pkl             # BC evaluation
â”‚   â”œâ”€â”€ cql_results.pkl            # CQL evaluation
â”‚   â”œâ”€â”€ dqn_results.pkl            # DQN evaluation
â”‚   â””â”€â”€ yalun_models_evaluation.pkl # Online RL evaluation
â”‚
â”œâ”€â”€ data/                           # Datasets
â”‚   â”œâ”€â”€ offline_dataset.pkl        # ~20K transitions for offline RL
â”‚   â””â”€â”€ checkpoint_ep*.pkl         # Training checkpoints
â”‚
â”œâ”€â”€ notebooks/                      # Jupyter notebooks
â”‚   â””â”€â”€ LEG_interplate_v2.ipynb    # LEG analysis notebook
â”‚
â”œâ”€â”€ gym-sepsis/                     # Sepsis simulation environment (submodule)
â”‚
â”œâ”€â”€ README.md                       # This file
â”œâ”€â”€ requirements.txt                # Python dependencies
â””â”€â”€ PAPER_UPDATE_GUIDE.md          # Paper update instructions
```

---

## ğŸ”¬ Methodology

### Algorithms Evaluated

#### Offline RL (No environment interaction during training)
1. **Behavior Cloning (BC)** - Supervised imitation learning baseline
2. **Conservative Q-Learning (CQL)** - Offline RL with conservatism penalty
3. **Deep Q-Network (DQN)** - Q-learning with deep networks (offline mode)

#### Online RL (Environment interaction during training)
4. **Double DQN with Attention** - Attention mechanism for feature selection
5. **Double DQN with Residual** - Residual connections for gradient flow
6. **Soft Actor-Critic (SAC)** - Maximum entropy RL

#### Baselines
7. **Random Policy** - Uniform random action selection
8. **Heuristic Policy** - Clinical guideline-based rules

### Interpretability Analysis

**Method:** Linearly Estimated Gradients (LEG) - A perturbation-based method that approximates Q-function gradients via local linear regression

**Metrics:**
- Maximum saliency magnitude (strength of strongest feature signal)
- Saliency range (differentiation between important/unimportant features)
- Clinical coherence (alignment with medical knowledge)

**Implementation:**
- 1,000 perturbation samples per state
- Standard deviation Ïƒ = 0.1
- Ridge regularization Î» = 10â»â¶
- 10 representative states per algorithm

---

## ğŸ“ˆ Figures

### Main Figures in Paper

1. **Figure 1: Algorithm Comparison** (`results/figures/algorithm_comparison.png`)
   - Overall survival rates
   - Average returns
   - SOFA-stratified survival
   - Episode lengths

2. **Figure 2: LEG Interpretability Comparison** (`results/figures/leg_interpretability_comparison.png`)
   - Maximum saliency magnitude comparison (600-fold difference!)
   - Feature importance patterns for CQL, BC, DQN
   - Clinical deployment suitability summary

### Additional Visualizations

- 90+ detailed LEG analysis figures in `results/figures/leg/`
- Individual state analysis for each algorithm
- Feature importance heatmaps
- Q-value landscapes

---

## ğŸ› ï¸ Technical Details

### Environment
- **Simulator:** gym-sepsis (based on MIMIC-III database)
- **State Space:** 46-dimensional (vital signs, lab values, SOFA scores)
- **Action Space:** 25 actions (5Ã—5 grid: IV fluids Ã— vasopressor dosing)
- **Episodes:** 500 evaluation episodes per algorithm

### Training
- **Offline Dataset:** ~20,000 transitions from heuristic policy
- **Offline Training:** BC (30 min), CQL (1-2 hours), DQN (1-2 hours)
- **Online Training:** 1M timesteps with environment interaction
- **Compute:** NVIDIA A100 GPU (Google Colab)

### Dependencies
```
Python 3.10
d3rlpy (offline RL)
stable-baselines3 (online RL)
gym==0.21.0
torch
numpy, pandas, matplotlib, seaborn
```

---

## ğŸ“š References

### Key Papers

1. **Raghu et al. (2017)** - "Deep Reinforcement Learning for Sepsis Treatment" - NeurIPS Workshop on ML for Health
2. **Kumar et al. (2020)** - "Conservative Q-Learning for Offline Reinforcement Learning" - NeurIPS
3. **Greydanus et al. (2018)** - "Visualizing and Understanding Atari Agents" - ICML (LEG method)

### Data and Code

- **MIMIC-III Database:** https://mimic.mit.edu/
- **gym-sepsis Environment:** https://github.com/gefeilin/gym-sepsis/tree/main/gym_sepsis/envs
- **This Project:** https://github.com/tutucheng99/STAT_8289_Project_1_Zhiyu_Cheng

---

## ğŸ‘¥ Author Contributions

- **Zhiyu Cheng**: Designed and implemented offline RL experiments (BC, CQL, DQN); conducted baseline evaluations; performed LEG interpretability analysis for offline methods; wrote Methods and Results sections; contributed to Discussion and Conclusion.

- **Yalun Ding**: Designed and implemented online RL experiments (DDQN-Attention, DDQN-Residual, SAC); conducted SOFA-stratified analysis; performed LEG interpretability analysis for online methods; wrote Introduction and Related Work sections; contributed to Discussion.

- **Chuanhui Peng**: Managed offline dataset generation and quality control; coordinated cross-algorithm evaluation; created main comparison figures; wrote Abstract and Problem Formulation sections; managed bibliography and LaTeX formatting.

---

## ğŸ“§ Contact

**Zhiyu Cheng** - George Washington University
**Course:** STAT 8289 - Reinforcement Learning
**Instructor:** [Instructor Name]
**Semester:** Fall 2025

---

## ğŸ“œ License

This project is for **academic use only**.

- Based on MIMIC-III database (PhysioNet Credentialed Health Data License)
- Uses gym-sepsis environment (MIT License)
- Code available for educational and research purposes

---

## ğŸ™ Acknowledgments

- STAT 8289 course staff at George Washington University
- Authors of gym-sepsis simulation environment
- MIMIC-III database contributors
- d3rlpy and Stable-Baselines3 library developers

---

**Project Status:** âœ… Complete
**Paper Status:** âœ… Final (29 pages)
**Last Updated:** October 28, 2025
