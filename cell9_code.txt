# STEP 4: Comprehensive Policy Evaluation
print('\n' + '='*80)
print('STEP 4: Evaluating Policies with Comprehensive Metrics')
print('='*80)

import gym_sepsis
import gym

class PolicyEvaluator:
    """Comprehensive policy evaluation for sepsis treatment."""
    
    def __init__(self, env_name='sepsis-v0'):
        self.env_name = env_name
    
    def evaluate_policy(self, model, n_episodes=100, verbose=False):
        """
        Evaluate a policy with comprehensive metrics.
        
        Args:
            model: Trained RL model
            n_episodes: Number of episodes to evaluate
            verbose: Print progress
            
        Returns:
            Dictionary of evaluation metrics
        """
        env = gym.make(self.env_name)
        
        # Metrics storage
        episode_rewards = []
        episode_lengths = []
        survival_outcomes = []  # 1 for survival, 0 for death
        action_counts = np.zeros(24)  # 24 actions (5x5 grid - 1)
        iv_usage = []  # Track IV fluid usage
        vp_usage = []  # Track vasopressor usage
        
        # State tracking
        initial_sofa_scores = []
        final_sofa_scores = []
        sofa_improvements = []
        
        for episode in range(n_episodes):
            state = env.reset()
            done = False
            episode_reward = 0
            episode_length = 0
            episode_actions = []
            
            # Track initial SOFA score (assuming it's at index 37)
            initial_sofa = state[37] if len(state) > 37 else None
            
            while not done:
                # Get action from model (handle any state structure)
                def flatten_state(s):
                    """Recursively flatten state handling any nested structure"""
                    if isinstance(s, dict):
                        # Flatten dictionary values in sorted key order
                        flattened = []
                        for k in sorted(s.keys()):
                            flattened.append(flatten_state(s[k]))
                        return np.concatenate(flattened) if flattened else np.array([], dtype=np.float32)
                    elif isinstance(s, (list, tuple)):
                        # Flatten list/tuple elements
                        flattened = [flatten_state(item) for item in s]
                        return np.concatenate(flattened) if flattened else np.array([], dtype=np.float32)
                    elif isinstance(s, np.ndarray):
                        # Flatten numpy array
                        return s.flatten().astype(np.float32)
                    elif isinstance(s, (int, float, np.integer, np.floating)):
                        # Convert numeric scalar to array
                        return np.array([float(s)], dtype=np.float32)
                    else:
                        # Try to convert to float, skip if not possible
                        try:
                            return np.array([float(s)], dtype=np.float32)
                        except (TypeError, ValueError):
                            # Skip non-numeric values
                            return np.array([], dtype=np.float32)
                state_flat = flatten_state(state).reshape(1, -1)
                action = model.predict(state_flat)[0]
                episode_actions.append(action)
                action_counts[action] += 1
                
                # Decode action to IV and VP bins
                iv_bin = action // 5
                vp_bin = action % 5
                
                # Execute action
                # Execute action (handle both old and new Gym API)
                step_result = env.step(action)
                if len(step_result) == 5:
                    # New Gym API (v0.26+): obs, reward, terminated, truncated, info
                    next_state, reward, terminated, truncated, info = step_result
                    done = terminated or truncated
                else:
                    # Old Gym API: obs, reward, done, info
                    next_state, reward, done, info = step_result
                episode_reward += reward
                episode_length += 1
                
                state = next_state
            
            # Track final SOFA score
            final_sofa = state[37] if len(state) > 37 else None
            
            # Record episode metrics
            episode_rewards.append(episode_reward)
            episode_lengths.append(episode_length)
            
            # Survival: in sepsis env, typically reward > 0 means survival
            survival = 1 if episode_reward > 0 else 0
            survival_outcomes.append(survival)
            
            # Calculate action statistics for this episode
            episode_iv = np.mean([a // 5 for a in episode_actions])
            episode_vp = np.mean([a % 5 for a in episode_actions])
            iv_usage.append(episode_iv)
            vp_usage.append(episode_vp)
            
            # SOFA score tracking
            if initial_sofa is not None and final_sofa is not None:
                initial_sofa_scores.append(initial_sofa)
                final_sofa_scores.append(final_sofa)
                sofa_improvements.append(initial_sofa - final_sofa)
            
            if verbose and (episode + 1) % 20 == 0:
                print(f"  Episode {episode + 1}/{n_episodes} completed")
        
        env.close()
        
        # Compute comprehensive metrics
        metrics = {
            # Reward metrics
            'mean_reward': np.mean(episode_rewards),
            'std_reward': np.std(episode_rewards),
            'median_reward': np.median(episode_rewards),
            'min_reward': np.min(episode_rewards),
            'max_reward': np.max(episode_rewards),
            
            # Episode length metrics
            'mean_episode_length': np.mean(episode_lengths),
            'std_episode_length': np.std(episode_lengths),
            'median_episode_length': np.median(episode_lengths),
            
            # Clinical outcome metrics
            'survival_rate': np.mean(survival_outcomes) * 100,  # Percentage
            'mortality_rate': (1 - np.mean(survival_outcomes)) * 100,
            
            # Treatment intensity metrics
            'mean_iv_usage': np.mean(iv_usage),
            'std_iv_usage': np.std(iv_usage),
            'mean_vp_usage': np.mean(vp_usage),
            'std_vp_usage': np.std(vp_usage),
            
            # SOFA score metrics (disease severity)
            'mean_initial_sofa': np.mean(initial_sofa_scores) if initial_sofa_scores else None,
            'mean_final_sofa': np.mean(final_sofa_scores) if final_sofa_scores else None,
            'mean_sofa_improvement': np.mean(sofa_improvements) if sofa_improvements else None,
            
            # Action distribution
            'action_distribution': action_counts / action_counts.sum(),
            'most_common_action': int(np.argmax(action_counts)),
            'action_entropy': -np.sum((action_counts / action_counts.sum()) * 
                                     np.log(action_counts / action_counts.sum() + 1e-10)),
            
            # Raw data for further analysis
            'episode_rewards': episode_rewards,
            'episode_lengths': episode_lengths,
            'survival_outcomes': survival_outcomes,
        }
        
        return metrics
    
    def compare_policies(self, models_dict, n_episodes=100):
        """
        Compare multiple policies.
        
        Args:
            models_dict: Dictionary of {model_name: model}
            n_episodes: Number of episodes per model
            
        Returns:
            Dictionary of {model_name: metrics}
        """
        all_metrics = {}
        
        print(f"\nEvaluating {len(models_dict)} policies over {n_episodes} episodes each...\n")
        
        for model_name, model in models_dict.items():
            print(f"{'='*80}")
            print(f"Evaluating: {model_name}")
            print('='*80)
            
            metrics = self.evaluate_policy(model, n_episodes=n_episodes, verbose=True)
            all_metrics[model_name] = metrics
            
            # Print summary
            print(f"\nüìä Results for {model_name}:")
            print(f"  Mean Reward: {metrics['mean_reward']:.2f} ¬± {metrics['std_reward']:.2f}")
            print(f"  Survival Rate: {metrics['survival_rate']:.1f}%")
            print(f"  Mean Episode Length: {metrics['mean_episode_length']:.1f}")
            print(f"  Mean IV Usage: {metrics['mean_iv_usage']:.2f}")
            print(f"  Mean VP Usage: {metrics['mean_vp_usage']:.2f}")
            if metrics['mean_sofa_improvement'] is not None:
                print(f"  Mean SOFA Improvement: {metrics['mean_sofa_improvement']:.2f}")
            print(f"  Action Entropy: {metrics['action_entropy']:.2f}")
            print()
        
        return all_metrics
    
    def plot_comparison(self, all_metrics, save_path=None):
        """Create comprehensive comparison visualizations."""
        # Safety check
        if not all_metrics:
            print("‚ö†Ô∏è  No evaluation data to plot.")
            return
        
        # Safety check
        if not all_metrics:
            print("‚ö†Ô∏è  No evaluation data to plot.")
            return
        
        model_names = list(all_metrics.keys())
        n_models = len(model_names)
        
        fig = plt.figure(figsize=(20, 12))
        gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)
        
        # 1. Reward Distribution
        ax1 = fig.add_subplot(gs[0, 0])
        rewards_data = [all_metrics[name]['episode_rewards'] for name in model_names]
        bp = ax1.boxplot(rewards_data, labels=model_names, patch_artist=True)
        for patch, color in zip(bp['boxes'], plt.cm.Set3(range(n_models))):
            patch.set_facecolor(color)
        ax1.set_ylabel('Episode Reward', fontweight='bold')
        ax1.set_title('Reward Distribution', fontweight='bold')
        ax1.grid(axis='y', alpha=0.3)
        plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45, ha='right')
        
        # 2. Survival Rate Comparison
        ax2 = fig.add_subplot(gs[0, 1])
        survival_rates = [all_metrics[name]['survival_rate'] for name in model_names]
        colors = plt.cm.Set3(range(n_models))
        bars = ax2.bar(range(n_models), survival_rates, color=colors, alpha=0.8, edgecolor='black')
        ax2.set_ylabel('Survival Rate (%)', fontweight='bold')
        ax2.set_title('Survival Rate Comparison', fontweight='bold')
        ax2.set_xticks(range(n_models))
        ax2.set_xticklabels(model_names, rotation=45, ha='right')
        ax2.set_ylim([0, 100])
        ax2.grid(axis='y', alpha=0.3)
        # Add value labels
        for i, (bar, val) in enumerate(zip(bars, survival_rates)):
            ax2.text(i, val + 2, f'{val:.1f}%', ha='center', fontweight='bold')
        
        # 3. Mean Reward with Error Bars
        ax3 = fig.add_subplot(gs[0, 2])
        means = [all_metrics[name]['mean_reward'] for name in model_names]
        stds = [all_metrics[name]['std_reward'] for name in model_names]
        ax3.bar(range(n_models), means, yerr=stds, color=colors, alpha=0.8, 
                edgecolor='black', capsize=5)
        ax3.set_ylabel('Mean Reward', fontweight='bold')
        ax3.set_title('Mean Reward ¬± Std', fontweight='bold')
        ax3.set_xticks(range(n_models))
        ax3.set_xticklabels(model_names, rotation=45, ha='right')
        ax3.grid(axis='y', alpha=0.3)
        ax3.axhline(y=0, color='red', linestyle='--', linewidth=1, alpha=0.5)
        
        # 4. Episode Length Distribution
        ax4 = fig.add_subplot(gs[1, 0])
        length_data = [all_metrics[name]['episode_lengths'] for name in model_names]
        bp2 = ax4.boxplot(length_data, labels=model_names, patch_artist=True)
        for patch, color in zip(bp2['boxes'], plt.cm.Set3(range(n_models))):
            patch.set_facecolor(color)
        ax4.set_ylabel('Episode Length', fontweight='bold')
        ax4.set_title('Episode Length Distribution', fontweight='bold')
        ax4.grid(axis='y', alpha=0.3)
        plt.setp(ax4.xaxis.get_majorticklabels(), rotation=45, ha='right')
        
        # 5. Treatment Intensity (IV vs VP)
        ax5 = fig.add_subplot(gs[1, 1])
        x = np.arange(n_models)
        width = 0.35
        iv_means = [all_metrics[name]['mean_iv_usage'] for name in model_names]
        vp_means = [all_metrics[name]['mean_vp_usage'] for name in model_names]
        ax5.bar(x - width/2, iv_means, width, label='IV Fluid', color='steelblue', alpha=0.8)
        ax5.bar(x + width/2, vp_means, width, label='Vasopressor', color='coral', alpha=0.8)
        ax5.set_ylabel('Mean Treatment Level', fontweight='bold')
        ax5.set_title('Treatment Intensity', fontweight='bold')
        ax5.set_xticks(x)
        ax5.set_xticklabels(model_names, rotation=45, ha='right')
        ax5.legend()
        ax5.grid(axis='y', alpha=0.3)
        
        # 6. Action Entropy
        ax6 = fig.add_subplot(gs[1, 2])
        entropies = [all_metrics[name]['action_entropy'] for name in model_names]
        ax6.bar(range(n_models), entropies, color=colors, alpha=0.8, edgecolor='black')
        ax6.set_ylabel('Action Entropy', fontweight='bold')
        ax6.set_title('Policy Diversity (Action Entropy)', fontweight='bold')
        ax6.set_xticks(range(n_models))
        ax6.set_xticklabels(model_names, rotation=45, ha='right')
        ax6.grid(axis='y', alpha=0.3)
        
        # 7. SOFA Score Improvement
        ax7 = fig.add_subplot(gs[2, 0])
        sofa_improvements = [all_metrics[name]['mean_sofa_improvement'] 
                            for name in model_names 
                            if all_metrics[name]['mean_sofa_improvement'] is not None]
        if sofa_improvements:
            ax7.bar(range(len(sofa_improvements)), sofa_improvements, 
                   color=colors[:len(sofa_improvements)], alpha=0.8, edgecolor='black')
            ax7.set_ylabel('SOFA Score Improvement', fontweight='bold')
            ax7.set_title('Disease Severity Improvement', fontweight='bold')
            ax7.set_xticks(range(len(sofa_improvements)))
            ax7.set_xticklabels(model_names[:len(sofa_improvements)], rotation=45, ha='right')
            ax7.axhline(y=0, color='red', linestyle='--', linewidth=1)
            ax7.grid(axis='y', alpha=0.3)
        
        # 8. Action Distribution Heatmap
        ax8 = fig.add_subplot(gs[2, 1:])
        action_data = np.array([all_metrics[name]['action_distribution'] 
                               for name in model_names])
        im = ax8.imshow(action_data, aspect='auto', cmap='YlOrRd')
        ax8.set_yticks(range(n_models))
        ax8.set_yticklabels(model_names)
        ax8.set_xlabel('Action Index', fontweight='bold')
        ax8.set_title('Action Distribution Heatmap', fontweight='bold')
        plt.colorbar(im, ax=ax8, label='Probability')
        
        # Super title
        fig.suptitle('Comprehensive Policy Evaluation Comparison', 
                    fontsize=16, fontweight='bold', y=0.98)
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f'Saved comparison plot to {save_path}')
        
        plt.show()
    
    def print_statistical_comparison(self, all_metrics):
        """Print statistical comparison table."""
        from scipy import stats
        
        # Safety check
        if not all_metrics:
            print("‚ö†Ô∏è  No evaluation data for statistical comparison.")
            return
        
        model_names = list(all_metrics.keys())
        
        print('\n' + '='*80)
        print('STATISTICAL COMPARISON TABLE')
        print('='*80)
        
        print(f"\n{'Metric':<30} | " + " | ".join([f"{name:<20}" for name in model_names]))
        print('-'*100)
        
        metrics_to_compare = [
            ('Mean Reward', 'mean_reward', '.2f'),
            ('Survival Rate (%)', 'survival_rate', '.1f'),
            ('Mean Episode Length', 'mean_episode_length', '.1f'),
            ('Mean IV Usage', 'mean_iv_usage', '.2f'),
            ('Mean VP Usage', 'mean_vp_usage', '.2f'),
            ('Action Entropy', 'action_entropy', '.2f'),
        ]
        
        for metric_name, metric_key, fmt in metrics_to_compare:
            values = [all_metrics[name][metric_key] for name in model_names]
            value_str = " | ".join([f"{val:{fmt}}".ljust(20) for val in values])
            print(f"{metric_name:<30} | {value_str}")
        
        # Statistical significance tests
        if len(model_names) == 2:
            print('\n' + '='*80)
            print('STATISTICAL SIGNIFICANCE (T-Test)')
            print('='*80)
            
            rewards_1 = all_metrics[model_names[0]]['episode_rewards']
            rewards_2 = all_metrics[model_names[1]]['episode_rewards']
            
            t_stat, p_value = stats.ttest_ind(rewards_1, rewards_2)
            
            print(f"\nReward Comparison: {model_names[0]} vs {model_names[1]}")
            print(f"  t-statistic: {t_stat:.4f}")
            print(f"  p-value: {p_value:.4f}")
            if p_value < 0.05:
                winner = model_names[0] if np.mean(rewards_1) > np.mean(rewards_2) else model_names[1]
                print(f"  ‚úì Significant difference (p < 0.05): {winner} performs better")
            else:
                print(f"  ‚úó No significant difference (p >= 0.05)")
        
        print('='*80)

# Initialize evaluator
evaluator = PolicyEvaluator()

# Evaluate all loaded models
print(f"\nüéØ Evaluating {len(models)} model(s)...")
try:
    all_evaluation_metrics = evaluator.compare_policies(models, n_episodes=100)
except Exception as e:
    print(f"\n‚ùå ERROR during evaluation: {e}")
    print(f"   Error type: {type(e).__name__}")
    print("\n‚ö†Ô∏è  IMPORTANT: Please restart the Jupyter kernel to load the updated code!")
    print("   (Kernel ‚Üí Restart Kernel, then re-run cells from the beginning)")
    import traceback
    traceback.print_exc()
    # Create empty metrics to prevent downstream errors
    all_evaluation_metrics = {}

# Create comparison visualizations
print("\nüìä Creating comparison visualizations...")
evaluator.plot_comparison(all_evaluation_metrics, 
                         save_path='./sepsis_leg_analysis/policy_comparison.png')

# Print statistical comparison
evaluator.print_statistical_comparison(all_evaluation_metrics)

print("\n‚úì Policy evaluation complete!")
