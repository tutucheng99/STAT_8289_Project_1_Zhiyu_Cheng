% ============================================================
% RELATED WORK SECTION
% ============================================================

\section{Related Work}\label{sec:related}

Our work builds on three research areas: reinforcement learning for sepsis treatment, offline RL algorithms, and interpretability methods for RL policies.

% ============================================================
\subsection{Reinforcement Learning for Sepsis Treatment}\label{sec:related:sepsis}

Raghu et al. \citeyearpar{raghu2017sepsis_drl} pioneered deep RL for sepsis treatment using the MIMIC-III database, formulating treatment as a discrete-action MDP with a $5 \times 5$ action grid (IV fluid × vasopressor dosing). Their work established the gym-sepsis simulation environment we use in this study. Komorowski et al. \citeyearpar{komorowski2018ai_clinician} developed the AI Clinician using fitted Q-iteration, achieving 98\% survival in retrospective simulation. While these studies demonstrated high performance, \textbf{neither provided quantitative feature-attribution analysis}. Their interpretability assessments relied on visualizing aggregate action distributions, revealing \textit{what} the policy does but not \textit{why}—a critical gap for regulatory approval and clinical trust. Recent work by Yao et al. \citeyearpar{yao2021sepsis_cql} applied CQL to sepsis but did not evaluate policy interpretability systematically.


% ============================================================
\subsection{Offline Reinforcement Learning}\label{sec:related:offline}

Offline RL learns from fixed datasets without environment interaction, addressing distributional shift when learned policies select out-of-distribution (OOD) actions \citep{levine2020offline}. \textbf{Behavior Cloning (BC)} \citep{pomerleau1991bc} treats offline RL as supervised imitation learning, avoiding distributional shift but cannot improve beyond the behavioral policy. \textbf{Conservative Q-Learning (CQL)} \citep{kumar2020cql} adds a conservatism penalty that discourages high Q-values for OOD actions, providing safety guarantees for healthcare. CQL's penalty encourages simpler Q-function representations aligned with the behavioral policy—when the behavioral policy follows interpretable threshold rules, CQL may learn Q-functions with strong gradients detectable by saliency analysis. \textbf{Deep Q-Network (DQN)} \citep{mnih2015dqn} combines Q-learning with deep networks and experience replay; originally designed for online learning, it can be adapted to offline settings but tends to overestimate OOD action values.


% ============================================================
\subsection{Interpretability Methods in RL}\label{sec:related:interp}

Regulatory agencies require explainable AI for medical decision support \citep{holzinger2017xai_healthcare}, yet deep RL policies are notoriously opaque. Greydanus et al. \citeyearpar{greydanus2018leg} introduced \textbf{Linearly Estimated Gradients (LEG)}, a perturbation-based method that approximates Q-function gradients via local linear regression, producing saliency maps highlighting which features drive action selection. We adopt LEG because it is model-agnostic (enabling fair comparison across algorithms), produces quantitative saliency scores, and aligns with clinical intuition where clinicians weight physiological indicators. No prior healthcare RL work has systematically compared interpretability across algorithms using gradient-based methods.


% ============================================================
\subsection{Research Gap}\label{sec:related:gap}

Despite a decade of sepsis RL research achieving strong retrospective performance, quantitative interpretability evaluation remains absent. While offline RL methods differ fundamentally in handling distributional shift, no study has examined whether these algorithmic differences translate to interpretability differences. Our work addresses this gap by jointly benchmarking offline and online RL on both survival outcomes and LEG-based interpretability, providing the first quantitative evidence on the performance--interpretability trade-off across RL paradigms.

% End of Related Work section