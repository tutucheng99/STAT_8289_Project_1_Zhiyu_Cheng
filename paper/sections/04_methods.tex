% ============================================================
% METHODS SECTION
% ============================================================

\section{Methods}\label{sec:methods}

This section describes the experimental setup, including the simulation environment, the three offline RL algorithms evaluated (Behavior Cloning, Conservative Q-Learning, and Deep Q-Network), the LEG interpretability analysis method, and the evaluation protocol. We use the gym-sepsis benchmark environment \citep{raghu2017sepsis_drl} as a standardized testbed for comparing algorithm interpretability, enabling fair evaluation across methods on a common clinical decision-making task with established state/action representations.

% ============================================================
\subsection{Environment and Data}\label{sec:methods:env}

\subsubsection{Gym-Sepsis Simulator}

We use the Gym-Sepsis environment \citep{raghu2017sepsis_drl}, an OpenAI Gym-compatible reinforcement learning simulator for sepsis treatment in the intensive care unit (ICU). The simulator's state transition model, episode termination dynamics, and outcome model were trained on the MIMIC-III dataset \citep{johnson2016mimic3}, which contains de-identified clinical data from over 40,000 ICU admissions at Beth Israel Deaconess Medical Center between 2001 and 2012.

\textbf{State Space.} At each timestep, the state is a 46-dimensional vector spanning laboratory values (lactate, creatinine, platelet count, etc.), vital signs (blood pressure, heart rate, SpO$_2$, etc.), demographics (age, gender, race), clinical severity scores (SOFA, LODS, SIRS, qSOFA, Elixhauser), and treatment status (mechanical ventilation, blood culture). The SOFA score \citep{vincent1996sofa} ranges from 0--24, with higher values indicating greater organ dysfunction; we use SOFA for severity stratification in Section~\ref{sec:methods:eval}.

\textbf{Action Space.} The action space is defined by a discrete $5 \times 5$ grid over medical interventions, spanning intravenous (IV) fluid and maximum vasopressor (VP) dosage within each 4-hour window. Each drug type is discretized into quartile bins based on all non-zero dosages observed in the MIMIC-III data, with an additional bin 0 representing no medication. Specifically:
\begin{equation}
\text{action} = 5 \times \text{IV\_bin} + \text{VP\_bin}, \quad \text{IV\_bin}, \text{VP\_bin} \in \{0, 1, 2, 3, 4\}
\end{equation}
resulting in 25 possible actions encoded as integers from 0 to 24. For example, action 0 corresponds to (IV=0, VP=0, i.e., no treatment), while action 24 corresponds to (IV=4, VP=4, i.e., maximum dosages for both drugs).

\textbf{Episode Dynamics.} Each timestep corresponds to a 4-hour window in the ICU. An episode spans the entire ICU stay of a patient, with the time horizon determined by the length of the trajectory until discharge (survival) or death. Episodes terminate when the patient outcome is determined by the simulator's learned outcome model.

\textbf{Reward Function.} We adopt the simple sparse reward function for all experiments to focus on long-term outcomes rather than intermediate clinical signals. The immediate reward is defined as:
\begin{equation}
r_t = \begin{cases}
+15 & \text{if episode terminates in discharge (survival)} \\
-15 & \text{if episode terminates in death} \\
0 & \text{for all intermediate steps}
\end{cases}
\end{equation}

This reward structure encourages policies to maximize patient survival while avoiding unnecessary interventions. Alternative reward functions incorporating intermediate feedback (e.g., SOFA score changes, lactate levels \citep{raghu2017sepsis_drl}) were explored during preliminary experiments but are not the focus of this comparative study.

\subsubsection{Offline Training Dataset}

To enable offline RL training, we generated an offline dataset by rolling out a heuristic policy in the Gym-Sepsis simulator for 10,000 episodes. The heuristic policy was designed based on clinical guidelines \citep{rhodes2017ssc, seymour2017sepsis_criteria} with threshold-based decision rules. Specifically, the policy escalates IV fluid administration when systolic blood pressure falls below 100 mmHg or lactate exceeds 2.0 mmol/L, escalates vasopressor dosing when mean arterial pressure drops below 65 mmHg despite fluid resuscitation, and de-escalates treatment when hemodynamic stability is achieved (systolic blood pressure exceeding 120 mmHg and lactate below 2.0 mmol/L). This heuristic policy achieved a 94.6\% survival rate on 500 evaluation episodes, providing a strong behavioral policy for offline RL algorithms to learn from. The resulting dataset contains approximately 100,000 state-action-reward-next\_state transitions, with an average episode length of 10 timesteps.

\textbf{Data Partitioning.} To support reproducible offline RL training, we partition the 10,000 simulated episodes as follows: 9,000 episodes (90\%) are allocated to the training set for policy learning, 500 episodes (5\%) form the validation set for hyperparameter tuning and early stopping, and the remaining 500 episodes (5\%) constitute the test set for final evaluation. All results reported in Section~\ref{sec:results} are computed exclusively on the held-out test set to ensure unbiased performance estimates. BC and CQL are trained on the training set, with validation set performance monitored to prevent overfitting. For DQN, which collects data online, the same 500-episode test set is used for evaluation to enable fair comparison across algorithms.

% ============================================================
\subsection{Algorithms}\label{sec:methods:algos}

We compare three offline RL algorithms representing different learning paradigms: Behavior Cloning (supervised learning), Conservative Q-Learning (offline Q-learning), and Deep Q-Network (online RL adapted for offline evaluation). All algorithms use the same neural network architecture for fair comparison: a 3-layer multilayer perceptron (MLP) with hidden dimensions [256, 256, 128] and ReLU activations.

\subsubsection{Behavior Cloning (BC)}

Behavior Cloning treats offline RL as a supervised learning problem, training a policy to imitate the behavioral policy by minimizing the negative log-likelihood of observed actions \citep{pomerleau1991bc}. Formally, given a dataset $\mathcal{D} = \{(s_i, a_i)\}_{i=1}^N$ of state-action pairs, BC learns a policy $\pi_{\theta}(a|s)$ by solving:
\begin{equation}
\theta^* = \arg\min_{\theta} -\frac{1}{N} \sum_{i=1}^N \log \pi_{\theta}(a_i | s_i)
\end{equation}

BC is computationally efficient and stable, but suffers from distribution shift when the learned policy encounters states not well-represented in the offline dataset \citep{ross2010dagger}.

\textbf{Implementation.} We use d3rlpy's DiscreteBCConfig with batch size 1,024, learning rate $1 \times 10^{-3}$ (Adam), training for 50,000 gradient steps (10 epochs $\times$ 5,000 steps/epoch).

\subsubsection{Conservative Q-Learning (CQL)}

Conservative Q-Learning \citep{kumar2020cql} is an offline RL algorithm that learns a conservative Q-function to avoid overestimation on out-of-distribution actions. CQL augments the standard Bellman error with a conservatism penalty that pushes down Q-values for unseen actions while pushing up Q-values for actions in the dataset:
\begin{equation}
\min_Q \alpha \cdot \mathbb{E}_{s \sim \mathcal{D}} \left[ \log \sum_a \exp Q(s, a) - \mathbb{E}_{a \sim \pi_\beta} Q(s, a) \right] + \frac{1}{2} \mathbb{E}_{(s,a,r,s') \sim \mathcal{D}} \left[ (Q(s,a) - \mathcal{T}^\pi Q(s,a))^2 \right]
\end{equation}
where $\alpha$ controls the strength of the conservatism penalty, $\pi_\beta$ is the behavioral policy, and $\mathcal{T}^\pi$ is the Bellman operator.

The conservatism penalty encourages the learned Q-function to assign lower values to actions that were not taken by the behavioral policy, reducing the risk of selecting suboptimal actions due to Q-value overestimation. The policy is derived as $\pi(s) = \arg\max_a Q(s, a)$.

\textbf{Implementation.} We use d3rlpy's DiscreteCQLConfig with batch size 1,024, learning rate $3 \times 10^{-4}$ (Adam), $\alpha = 1.0$, target network updates every 2,000 steps, training for 200,000 gradient steps.

\subsubsection{Deep Q-Network (DQN)}

Deep Q-Network \citep{mnih2015dqn} is a foundational deep RL algorithm that combines Q-learning with deep neural networks. DQN uses two key techniques for stability: (1) experience replay, which stores transitions in a replay buffer and samples mini-batches for training, and (2) a target network $Q_{\theta^-}$ that is periodically synchronized with the main network $Q_\theta$ to stabilize Q-value targets.

The Q-function is updated to minimize the temporal difference (TD) error:
\begin{equation}
\mathcal{L}(\theta) = \mathbb{E}_{(s,a,r,s') \sim \mathcal{D}} \left[ \left( Q_\theta(s,a) - \left( r + \gamma \max_{a'} Q_{\theta^-}(s', a') \right) \right)^2 \right]
\end{equation}

The policy is derived greedily as $\pi(s) = \arg\max_a Q_\theta(s, a)$, with $\epsilon$-greedy exploration during training ($\epsilon$ annealed from 1.0 to 0.05).

\textbf{Implementation and Training Paradigm.} We use the Stable-Baselines3 library for DQN training. Unlike BC and CQL, which are designed explicitly for offline learning from a fixed dataset, DQN was trained \textit{online} by interacting with the Gym-Sepsis simulator and accumulating experience in a replay buffer of size 100,000. This methodological choice reflects DQN's original design as an online RL algorithm \citep{mnih2015dqn} and enables us to compare interpretability across both offline-specific methods (BC, CQL) and online methods adapted for safety-critical domains. We label our study as focusing on "offline RL" because BC and CQL are trained offline, and \textit{all three algorithms are evaluated identically in offline mode}â€”i.e., policies are tested on a held-out set of 500 episodes without further environment interaction. This evaluation protocol ensures fair comparison: DQN's online training provides it with potentially richer exploration data, yet it must still generalize to unseen test episodes in the same manner as offline-trained policies. Thus, our interpretability analysis reflects how each algorithm's learned representations (whether from offline or online training) manifest in deployment settings where no further learning occurs.

DQN uses batch size 256, learning rate $1 \times 10^{-4}$ (Adam), target network updates every 1,000 steps, $\epsilon$-greedy exploration (1.0 $\to$ 0.05), training for 100,000 timesteps.

\subsubsection{Online RL Algorithms}\label{sec:methods:algos:online}

To provide a comprehensive comparison between offline and online RL paradigms, we also evaluate three state-of-the-art online RL algorithms with architectural innovations (implemented by collaborator Y. Ding). Unlike the offline methods above, these algorithms train by interacting with the Gym-Sepsis simulator, collecting 1 million timesteps of experience through exploration. This comparison illuminates the performance-safety trade-off: online methods can explore beyond the behavioral policy's distribution but require environment access during trainingâ€”a significant constraint in clinical settings where patient safety prohibits trial-and-error learning.

\paragraph{Double DQN with Attention (DDQN-Attention).}
This algorithm extends Double DQN \citep{vanHasselt2016double_dqn} with a multi-head self-attention mechanism in the encoder network. Double DQN addresses Q-value overestimation by decoupling action selection and evaluation: the main network selects the best action, while the target network evaluates it. The attention layer allows the model to dynamically weight different state features based on their relevance to the current decision:
\begin{equation}
h_t = \text{MultiHeadAttention}(s_t, s_t, s_t) + s_t
\end{equation}
where the residual connection helps gradient flow during backpropagation. The attention mechanism computes scaled dot-product attention across 4 parallel heads, each learning different feature correlations. The encoder uses two hidden layers of 256 and 128 units respectively, with the attention layer inserted after the first hidden layer to capture high-level feature interactions.

\paragraph{Double DQN with Residual Connections (DDQN-Residual).}
This variant incorporates deep residual networks \citep{he2016deep} to enable training of deeper Q-networks without gradient vanishing. The architecture uses three hidden layers of 256 units each, with skip connections between layers:
\begin{equation}
h_{l+1} = \sigma(\text{LayerNorm}(W_l h_l + b_l + h_l))
\end{equation}
where $\sigma$ is the ReLU activation, $W_l$ and $b_l$ are learnable weights and biases, and the additive skip connection $h_l$ preserves gradient information. Layer normalization stabilizes training by normalizing activations within each layer. The residual architecture is hypothesized to learn more complex value functions by decomposing Q-value estimation into a base value plus incremental adjustments.

\paragraph{Soft Actor-Critic (SAC).}
SAC \citep{haarnoja2018soft} is a maximum entropy RL algorithm that optimizes both expected return and policy entropy, encouraging exploration and robustness. The objective function is:
\begin{equation}
J(\pi) = \mathbb{E}_{\tau \sim \pi}\left[\sum_t r(s_t, a_t) + \alpha \mathcal{H}(\pi(\cdot|s_t))\right]
\end{equation}
where $\mathcal{H}(\pi(\cdot|s))$ is the entropy of the policy at state $s$, and $\alpha$ is a temperature parameter that balances exploitation (maximizing reward) and exploration (maximizing entropy). We use the discrete action space variant of SAC with a residual encoder architecture (3 layers of 256 units with skip connections). The temperature $\alpha$ is automatically tuned during training using a dual gradient descent approach \citep{haarnoja2018soft_applications}, starting from $\alpha = 0.2$ and adjusting to maintain a target entropy equal to 95\% of the maximum entropy $\log(25)$ for the 25-action space.

\paragraph{Training Details.}
All three online RL algorithms were trained with 1,000,000 environment interaction steps using experience replay buffers of size 100,000. Training used batch size 256, learning rate $3 \times 10^{-4}$ (Adam), and target network soft updates with $\tau = 0.005$. Exploration for DDQN variants used $\epsilon$-greedy with $\epsilon$ annealed from 1.0 to 0.05 over the first 100,000 steps. Unlike offline methods which require only the pre-collected dataset, these algorithms necessitate access to the simulator during trainingâ€”a key distinction when considering deployment in clinical settings where patient safety prohibits exploratory interventions.

% ============================================================
\subsection{LEG Interpretability Analysis}\label{sec:methods:leg}

To assess interpretability, we employ Linearly Estimated Gradients (LEG) \citep{greydanus2018leg}, a model-agnostic perturbation-based method for computing feature importance in RL policies. LEG approximates the gradient $\nabla_s Q(s_0, \pi(s_0))$ by sampling perturbations around a given state and performing ridge regression on Q-value changes to obtain saliency scores $\hat{\gamma}_j$ for each feature $j$. We apply LEG to all algorithms using 1,000 perturbation samples per state, analyzing 10 representative states sampled uniformly across SOFA severity levels. We quantify interpretability using three metrics: maximum saliency magnitude (strength of strongest feature signal), saliency range (spread of importance across features), and clinical coherence (alignment with medical knowledge). Full mathematical formulation and implementation details are provided in Appendix~\ref{appendix:leg}.

% ============================================================
\subsection{Evaluation Metrics}\label{sec:methods:eval}

We evaluate algorithm performance using the following metrics:

\subsubsection{Primary Outcome Metrics}

We evaluate algorithm performance using three primary metrics. The \textbf{survival rate}, defined as the proportion of episodes ending in hospital discharge rather than death, serves as the primary clinical endpoint. The \textbf{average return} is computed as the mean cumulative reward across all evaluation episodes:
\begin{equation}
\bar{R} = \frac{1}{N} \sum_{i=1}^N \sum_{t=0}^{T_i} r_{i,t}
\end{equation}
where $N$ is the number of evaluation episodes and $T_i$ is the length of episode $i$. Finally, the \textbf{average episode length}, measured as the mean number of timesteps per episode, provides an indication of treatment duration and efficiency.

\subsubsection{SOFA-Stratified Analysis}

To assess algorithm performance across patient severity levels, we stratify evaluation episodes by baseline SOFA score into three clinically meaningful groups. Episodes with SOFA scores of 5 or less are classified as \textbf{low SOFA} (least severe patients), those with scores between 6 and 10 as \textbf{medium SOFA} (moderate severity), and those with scores of 11 or higher as \textbf{high SOFA} (most severe, highest mortality risk). For each stratum, we report survival rate and sample size. This stratification reveals whether algorithms excel on specific patient subgroups, which has important implications for clinical deployment, as policies that perform well overall but fail on high-severity patients may not be suitable for ICU use.

\subsubsection{Statistical Significance Testing}

We assess statistical significance of survival rate differences using a chi-square test for categorical outcomes across algorithms. Confidence intervals for survival rates are computed using the Wilson score interval with 95\% confidence level.

% ============================================================
\subsection{Baseline Policies}\label{sec:methods:baselines}

To contextualize RL algorithm performance, we evaluate two baseline policies:

\subsubsection{Random Policy}

The random policy selects actions uniformly at random from the 25-action space at each timestep, i.e., $\pi(a|s) = \frac{1}{25}$ for all $s, a$. This provides a lower bound on expected performance and tests the difficulty of the environment.

\subsubsection{Heuristic Policy}

The heuristic policy implements threshold-based clinical decision rules derived from sepsis treatment guidelines \citep{rhodes2017ssc}. The policy first extracts key physiological features including systolic blood pressure (SysBP), mean arterial blood pressure (MeanBP), lactate levels, and SOFA score. The IV fluid dosing bin is then determined hierarchically: if SysBP falls below 90 mmHg or lactate exceeds 4.0 mmol/L, the maximum IV fluid bin (4) is selected; otherwise, if SysBP is below 100 mmHg or lactate exceeds 2.0 mmol/L, bin 3 is chosen; if SysBP is below 110 mmHg, bin 2 is selected; and for stable patients, a maintenance fluid rate (bin 1) is used. Vasopressor dosing follows similar logic based on mean arterial pressure: bin 3 is selected when MeanBP drops below 65 mmHg (the clinical threshold for hypotension), bin 2 when MeanBP is between 65 and 70 mmHg, and no vasopressor (bin 0) is given when blood pressure is adequate. The final action is computed as $5 \times \text{IV\_bin} + \text{VP\_bin}$, encoding both treatment decisions into a single discrete action.

This heuristic policy achieved 94.6\% survival on 500 evaluation episodes, demonstrating that simple threshold-based rules perform surprisingly well in this simulator. However, as we will show, the heuristic lacks the adaptability and personalization that RL algorithms can provide.

\subsubsection{Evaluation Protocol}

All policies (random, heuristic, BC, CQL, DQN) are evaluated on 500 episodes in the Gym-Sepsis simulator using identical random seeds for reproducibility. Each episode is initialized with a random patient state sampled from the MIMIC-III-derived distribution. Policies are evaluated deterministically (no exploration noise) to assess their learned behavior.

% End of Methods section
