% ============================================================
% METHODS SECTION
% ============================================================

\section{Methods}\label{sec:methods}

We describe the experimental setup: the gym-sepsis simulation environment, offline and online RL algorithms, LEG interpretability analysis, and evaluation protocol.

% ============================================================
\subsection{Environment and Data}\label{sec:methods:env}

\subsubsection{Gym-Sepsis Simulator}

We use Gym-Sepsis \citep{raghu2017sepsis_drl}, an RL simulator for ICU sepsis treatment trained on MIMIC-III data \citep{johnson2016mimic3}.

\textbf{State Space.} At each timestep, the state is a 46-dimensional vector spanning laboratory values (lactate, creatinine, platelet count, etc.), vital signs (blood pressure, heart rate, SpO$_2$, etc.), demographics (age, gender, race), clinical severity scores (SOFA, LODS, SIRS, qSOFA, Elixhauser), and treatment status (mechanical ventilation, blood culture). The SOFA score \citep{vincent1996sofa} ranges from 0--24, with higher values indicating greater organ dysfunction; we use SOFA for severity stratification in Section~\ref{sec:methods:eval}.

\textbf{Action Space.} A discrete $5 \times 5$ grid over IV fluid and vasopressor dosage bins (action $ = 5 \times \text{IV\_bin} + \text{VP\_bin}$), yielding 25 actions.

\textbf{Episode Dynamics \& Reward.} Episodes span ICU stays (4-hour timesteps) until discharge or death. Sparse reward: $r_t = +15$ (survival), $-15$ (death), $0$ (intermediate).

\subsubsection{Offline Training Dataset}

We generated an offline dataset of 10,000 episodes (~100K transitions) using a heuristic policy based on clinical guidelines \citep{rhodes2017ssc, seymour2017sepsis_criteria}, achieving 94.6\% survival. Data partitioning: 9,000 train, 500 validation, 500 test episodes.

% ============================================================
\subsection{Algorithms}\label{sec:methods:algos}

We compare three offline RL algorithms representing different learning paradigms: Behavior Cloning (supervised learning), Conservative Q-Learning (offline Q-learning), and Deep Q-Network (online RL adapted for offline evaluation). All algorithms use the same neural network architecture for fair comparison: a 3-layer multilayer perceptron (MLP) with hidden dimensions [256, 256, 128] and ReLU activations.

\subsubsection{Behavior Cloning (BC)}

Behavior Cloning treats offline RL as a supervised learning problem, training a policy to imitate the behavioral policy by minimizing the negative log-likelihood of observed actions \citep{pomerleau1991bc}. Formally, given a dataset $\mathcal{D} = \{(s_i, a_i)\}_{i=1}^N$ of state-action pairs, BC learns a policy $\pi_{\theta}(a|s)$ by solving:
\begin{equation}
\theta^* = \arg\min_{\theta} -\frac{1}{N} \sum_{i=1}^N \log \pi_{\theta}(a_i | s_i)
\end{equation}

BC is computationally efficient and stable, but suffers from distribution shift when the learned policy encounters states not well-represented in the offline dataset \citep{ross2010dagger}.

\textbf{Implementation.} We use d3rlpy's DiscreteBCConfig with batch size 1,024, learning rate $1 \times 10^{-3}$ (Adam), training for 50,000 gradient steps (10 epochs $\times$ 5,000 steps/epoch).

\subsubsection{Conservative Q-Learning (CQL)}

Conservative Q-Learning \citep{kumar2020cql} is an offline RL algorithm that learns a conservative Q-function to avoid overestimation on out-of-distribution actions. CQL augments the standard Bellman error with a conservatism penalty that pushes down Q-values for unseen actions while pushing up Q-values for actions in the dataset:
\begin{equation}
\min_Q \alpha \cdot \mathbb{E}_{s \sim \mathcal{D}} \left[ \log \sum_a \exp Q(s, a) - \mathbb{E}_{a \sim \pi_\beta} Q(s, a) \right] + \frac{1}{2} \mathbb{E}_{(s,a,r,s') \sim \mathcal{D}} \left[ (Q(s,a) - \mathcal{T}^\pi Q(s,a))^2 \right]
\end{equation}
where $\alpha$ controls the strength of the conservatism penalty, $\pi_\beta$ is the behavioral policy, and $\mathcal{T}^\pi$ is the Bellman operator.

The conservatism penalty encourages the learned Q-function to assign lower values to actions that were not taken by the behavioral policy, reducing the risk of selecting suboptimal actions due to Q-value overestimation. The policy is derived as $\pi(s) = \arg\max_a Q(s, a)$.

\textbf{Implementation.} We use d3rlpy's DiscreteCQLConfig with batch size 1,024, learning rate $3 \times 10^{-4}$ (Adam), $\alpha = 1.0$, target network updates every 2,000 steps, training for 200,000 gradient steps.

\subsubsection{Deep Q-Network (DQN)}

Deep Q-Network \citep{mnih2015dqn} is a foundational deep RL algorithm that combines Q-learning with deep neural networks. DQN uses two key techniques for stability: (1) experience replay, which stores transitions in a replay buffer and samples mini-batches for training, and (2) a target network $Q_{\theta^-}$ that is periodically synchronized with the main network $Q_\theta$ to stabilize Q-value targets.

The Q-function is updated to minimize the temporal difference (TD) error:
\begin{equation}
\mathcal{L}(\theta) = \mathbb{E}_{(s,a,r,s') \sim \mathcal{D}} \left[ \left( Q_\theta(s,a) - \left( r + \gamma \max_{a'} Q_{\theta^-}(s', a') \right) \right)^2 \right]
\end{equation}

The policy is derived greedily as $\pi(s) = \arg\max_a Q_\theta(s, a)$, with $\epsilon$-greedy exploration during training ($\epsilon$ annealed from 1.0 to 0.05).

\textbf{Implementation and Training Paradigm.} We use the Stable-Baselines3 library for DQN training. Unlike BC and CQL, which are designed explicitly for offline learning from a fixed dataset, DQN was trained \textit{online} by interacting with the Gym-Sepsis simulator and accumulating experience in a replay buffer of size 100,000. This methodological choice reflects DQN's original design as an online RL algorithm \citep{mnih2015dqn} and enables us to compare interpretability across both offline-specific methods (BC, CQL) and online methods adapted for safety-critical domains. We label our study as focusing on "offline RL" because BC and CQL are trained offline, and \textit{all three algorithms are evaluated identically in offline mode}—i.e., policies are tested on a held-out set of 500 episodes without further environment interaction. This evaluation protocol ensures fair comparison: DQN's online training provides it with potentially richer exploration data, yet it must still generalize to unseen test episodes in the same manner as offline-trained policies. Thus, our interpretability analysis reflects how each algorithm's learned representations (whether from offline or online training) manifest in deployment settings where no further learning occurs.

DQN uses batch size 256, learning rate $1 \times 10^{-4}$ (Adam), target network updates every 1,000 steps, $\epsilon$-greedy exploration (1.0 $\to$ 0.05), training for 100,000 timesteps.

\subsubsection{Online RL Algorithms}\label{sec:methods:algos:online}

To provide a comprehensive comparison between offline and online RL paradigms, we also evaluate three state-of-the-art online RL algorithms with architectural innovations (implemented by collaborator Y. Ding). Unlike the offline methods above, these algorithms train by interacting with the Gym-Sepsis simulator, collecting 1 million timesteps of experience through exploration. This comparison illuminates the performance-safety trade-off: online methods can explore beyond the behavioral policy's distribution but require environment access during training—a significant constraint in clinical settings where patient safety prohibits trial-and-error learning.

\paragraph{Double DQN with Attention (DDQN-Attention).}
This algorithm extends Double DQN \citep{vanHasselt2016double_dqn} with a multi-head self-attention mechanism in the encoder network. Double DQN addresses Q-value overestimation by decoupling action selection and evaluation: the main network selects the best action, while the target network evaluates it. The attention layer allows the model to dynamically weight different state features based on their relevance to the current decision:
\begin{equation}
h_t = \text{MultiHeadAttention}(s_t, s_t, s_t) + s_t
\end{equation}
where the residual connection helps gradient flow during backpropagation. The attention mechanism computes scaled dot-product attention across 4 parallel heads, each learning different feature correlations. The encoder uses two hidden layers of 256 and 128 units respectively, with the attention layer inserted after the first hidden layer to capture high-level feature interactions.

\paragraph{Double DQN with Residual Connections (DDQN-Residual).}
This variant incorporates deep residual networks \citep{he2016deep} to enable training of deeper Q-networks without gradient vanishing. The architecture uses three hidden layers of 256 units each, with skip connections between layers:
\begin{equation}
h_{l+1} = \sigma(\text{LayerNorm}(W_l h_l + b_l + h_l))
\end{equation}
where $\sigma$ is the ReLU activation, $W_l$ and $b_l$ are learnable weights and biases, and the additive skip connection $h_l$ preserves gradient information. Layer normalization stabilizes training by normalizing activations within each layer. The residual architecture is hypothesized to learn more complex value functions by decomposing Q-value estimation into a base value plus incremental adjustments.

\paragraph{Soft Actor-Critic (SAC).}
SAC \citep{haarnoja2018soft} is a maximum entropy RL algorithm that optimizes both expected return and policy entropy, encouraging exploration and robustness. The objective function is:
\begin{equation}
J(\pi) = \mathbb{E}_{\tau \sim \pi}\left[\sum_t r(s_t, a_t) + \alpha \mathcal{H}(\pi(\cdot|s_t))\right]
\end{equation}
where $\mathcal{H}(\pi(\cdot|s))$ is the entropy of the policy at state $s$, and $\alpha$ is a temperature parameter that balances exploitation (maximizing reward) and exploration (maximizing entropy). We use the discrete action space variant of SAC with a residual encoder architecture (3 layers of 256 units with skip connections). The temperature $\alpha$ is automatically tuned during training using a dual gradient descent approach \citep{haarnoja2018soft_applications}, starting from $\alpha = 0.2$ and adjusting to maintain a target entropy equal to 95\% of the maximum entropy $\log(25)$ for the 25-action space.

\paragraph{Training Details.}
All three online RL algorithms were trained with 1,000,000 environment interaction steps using experience replay buffers of size 100,000. Training used batch size 256, learning rate $3 \times 10^{-4}$ (Adam), and target network soft updates with $\tau = 0.005$. Exploration for DDQN variants used $\epsilon$-greedy with $\epsilon$ annealed from 1.0 to 0.05 over the first 100,000 steps. Unlike offline methods which require only the pre-collected dataset, these algorithms necessitate access to the simulator during training—a key distinction when considering deployment in clinical settings where patient safety prohibits exploratory interventions.

% ============================================================
\subsection{LEG Interpretability Analysis}\label{sec:methods:leg}

To assess interpretability, we employ Linearly Estimated Gradients (LEG) \citep{greydanus2018leg}, a model-agnostic perturbation-based method for computing feature importance in RL policies. LEG approximates the gradient $\nabla_s Q(s_0, \pi(s_0))$ by sampling perturbations around a given state and performing ridge regression on Q-value changes to obtain saliency scores $\hat{\gamma}_j$ for each feature $j$. We apply LEG to all six algorithms---three offline methods (BC, CQL, DQN) and three online methods (DDQN-Attention, DDQN-Residual, SAC)---using identical parameters: 1,000 perturbation samples per state ($\sigma = 0.1$), analyzing 10 representative states sampled uniformly across SOFA severity levels. We quantify interpretability using three metrics: maximum saliency magnitude (strength of strongest feature signal), saliency range (spread of importance across features), and clinical coherence (alignment with medical knowledge). Full mathematical formulation and implementation details are provided in Appendix~\ref{appendix:leg}.

% ============================================================
\subsection{Evaluation Metrics}\label{sec:methods:eval}

We evaluate algorithm performance using the following metrics:

\subsubsection{Primary Outcome Metrics}

\textbf{Survival rate} (proportion of episodes ending in discharge), \textbf{average return} ($\bar{R} = \frac{1}{N} \sum_i \sum_t r_{i,t}$), and \textbf{average episode length}.

\subsubsection{SOFA-Stratified Analysis}

Episodes stratified by SOFA score: \textbf{low} ($\leq$5), \textbf{medium} (6-10), \textbf{high} ($\geq$11), reporting survival rate per stratum.

\subsubsection{Statistical Significance Testing}

We assess statistical significance of survival rate differences using a chi-square test for categorical outcomes across algorithms. Confidence intervals for survival rates are computed using the Wilson score interval with 95\% confidence level.

% ============================================================
\subsection{Baseline Policies}\label{sec:methods:baselines}

To contextualize RL algorithm performance, we evaluate two baseline policies:

\subsubsection{Random Policy}

The random policy selects actions uniformly at random from the 25-action space at each timestep, i.e., $\pi(a|s) = \frac{1}{25}$ for all $s, a$. This provides a lower bound on expected performance and tests the difficulty of the environment.

\subsubsection{Heuristic Policy}

Implements threshold-based rules from sepsis guidelines \citep{rhodes2017ssc}: escalate IV fluids when SysBP$<$100 mmHg or lactate$>$2.0 mmol/L; escalate vasopressors when MeanBP$<$65 mmHg. Achieved 94.6\% survival.

\subsubsection{Evaluation Protocol}

All policies (random, heuristic, BC, CQL, DQN) are evaluated on 500 episodes in the Gym-Sepsis simulator using identical random seeds for reproducibility. Each episode is initialized with a random patient state sampled from the MIMIC-III-derived distribution. Policies are evaluated deterministically (no exploration noise) to assess their learned behavior.

% End of Methods section
