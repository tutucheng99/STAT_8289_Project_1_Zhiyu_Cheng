% ============================================================
% RESULTS SECTION
% ============================================================

\section{Results}\label{sec:results}

We present the evaluation results for all policies across 500 episodes each, focusing on overall performance, SOFA-stratified analysis, and most importantly, the LEG interpretability comparison that reveals dramatic differences in feature importance patterns across algorithms.

% ============================================================
\subsection{Overall Performance Comparison}\label{sec:results:overall}

Table \ref{tab:overall-performance} and Figure \ref{fig:algorithm-comparison} summarize the performance of all eight policies evaluated in this study: two baselines (random and heuristic), three offline RL algorithms (BC, CQL, DQN), and three online RL algorithms (DDQN-Attention, DDQN-Residual, SAC). Among all methods, DDQN-Attention achieves the highest survival rate at 95.4\%, demonstrating the benefit of attention mechanisms for feature selection in complex medical domains. SAC achieves 94.8\% survival, while DDQN-Residual achieves 94.2\%, comparable to BC (94.2\%). The random and heuristic baselines achieve 95.0\% and 94.6\% respectively, demonstrating that simple threshold-based clinical rules are remarkably effective in this simulation environment. All methods fall within a narrow 1.4 percentage point range (94.0--95.4\%), with online RL methods achieving marginally higher survival rates than offline RL (95.4\%, 94.8\%, 94.2\% vs. 94.2\%, 94.0\%, 94.0\%).

\textbf{Random Policy Paradox.} The counterintuitive finding that random action selection achieves the highest survival rate (95.0\%) warrants explanation. This result likely reflects an artifact of the Gym-Sepsis simulator's dynamics rather than a substantive finding about sepsis treatment. The simulator, trained on MIMIC-III data, may have learned a relatively forgiving outcome model where patient survival is robust to treatment variation, particularly when treatment actions remain within reasonable ranges (as random selection from the 25-action grid ensures). Additionally, the sparse reward structure provides no intermediate feedback to penalize suboptimal actions during treatment, allowing even poorly targeted interventions to succeed if they avoid extreme under-treatment or over-treatment. This high baseline survival rate ($\sim$94--95\% across all policies) underscores the limitations of using simulator-only evaluation and highlights the need for real-world validation where treatment quality more decisively affects outcomes.

Average cumulative returns mirror the survival rate patterns: DDQN-Attention achieves the highest return at 13.62 $\pm$ 6.28, followed by SAC at 13.44 $\pm$ 6.66, random at 13.50 $\pm$ 6.54, and heuristic at 13.38 $\pm$ 6.78. Offline RL methods achieve slightly lower returns: BC at 13.26 $\pm$ 7.01, CQL and DQN both at 13.20 $\pm$ 7.12. The high standard deviations reflect the sparse reward structure, where episodes yield either +15 (survival) or -15 (death) with minimal intermediate rewards. Average episode lengths vary from 7.7 timesteps (SAC) to 9.5 timesteps (BC, CQL, heuristic), with online RL methods generally achieving shorter episodes (7.7--9.0) compared to offline methods (7.8--9.5). The shorter episode lengths for online RL and offline DQN suggest that these algorithms may have learned more aggressive treatment strategies that accelerate patient discharge, though this does not translate to substantially improved survival outcomes.

However, this modest performance gain for online RL (1.2--1.4 percentage points over the best offline method) comes at a significant practical cost: online methods require extensive environment interaction during training (1 million timesteps)—infeasible in clinical settings where patient safety prohibits trial-and-error learning. In contrast, offline RL methods achieve comparable survival rates (94.0--94.2\%) using only pre-collected data, with no patient risk during training. This small performance gap, combined with the safety constraints of clinical deployment, motivates our focus on interpretability as a critical secondary criterion for algorithm selection. As we will demonstrate in Section~\ref{sec:results:leg}, offline methods—particularly CQL—offer superior interpretability without sacrificing performance, making them more suitable for clinical decision support systems.

\begin{table}[htbp]
\centering
\caption{Overall performance comparison across baseline and RL policies over 500 evaluation episodes. All methods achieve similar overall survival rates (94.0--95.4\%). Online RL methods (DDQN-Attention, DDQN-Residual, SAC) achieve marginally higher survival rates, with DDQN-Attention reaching the highest at 95.4\%, but this comes at the cost of requiring environment interaction during training.}
\label{tab:overall-performance}
\begin{tabular}{lcccc}
\toprule
\textbf{Model} & \textbf{Survival (\%)} & \textbf{Avg Return} & \textbf{Avg Length} & \textbf{Paradigm} \\
\midrule
\multicolumn{5}{l}{\textit{Baselines}} \\
Random          & 95.0               & 13.50 $\pm$ 6.54    & 9.3 $\pm$ 1.1       & -- \\
Heuristic       & 94.6               & 13.38 $\pm$ 6.78    & 9.5 $\pm$ 1.2       & -- \\
\midrule
\multicolumn{5}{l}{\textit{Offline RL}} \\
BC              & 94.2               & 13.26 $\pm$ 7.01    & 9.5 $\pm$ 0.6       & Offline \\
CQL             & 94.0               & 13.20 $\pm$ 7.12    & 9.5 $\pm$ 0.5       & Offline \\
DQN             & 94.0               & 13.20 $\pm$ 7.12    & 7.8 $\pm$ 1.2       & Offline \\
\midrule
\multicolumn{5}{l}{\textit{Online RL}} \\
DDQN-Attention  & \textbf{95.4}      & 13.62 $\pm$ 6.28    & 7.9 $\pm$ 1.0       & Online \\
DDQN-Residual   & 94.2               & 13.26 $\pm$ 7.01    & 9.0 $\pm$ 0.8       & Online \\
SAC             & 94.8               & 13.44 $\pm$ 6.66    & 7.7 $\pm$ 1.2       & Online \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[htbp]
\centering
\includegraphics[width=\textwidth]{../results/figures/algorithm_comparison.png}
\caption{Comprehensive performance comparison across baseline and RL policies. \textbf{Top Left:} Overall survival rates show narrow range (94.0--95.4\%) with DDQN-Attention achieving highest at 95.4\%. \textbf{Top Right:} Average returns with large standard deviations reflect sparse reward structure. \textbf{Bottom Left:} SOFA-stratified survival rates demonstrate ceiling effect for low/medium severity patients, with meaningful differences emerging only for high-SOFA patients. \textbf{Bottom Right:} Episode lengths vary from 7.7 (SAC) to 9.5 timesteps (BC, CQL, Heuristic), suggesting different treatment strategies.}
\label{fig:algorithm-comparison}
\end{figure}


% ============================================================
\subsection{SOFA-Stratified Analysis}\label{sec:results:sofa}

To understand whether algorithms differ in their ability to treat patients of varying severity, we stratified evaluation episodes by baseline SOFA score into three groups: low SOFA ($\leq$ 5), medium SOFA (6--10), and high SOFA ($\geq$ 11). While all methods achieve excellent survival rates exceeding 97\% on low- and medium-severity patients (ceiling effect), the most clinically meaningful distinctions emerge for high-severity patients (SOFA $\geq$ 11), who face substantially elevated mortality risk. Table \ref{tab:sofa-stratified} focuses on this critical subgroup, presenting detailed performance metrics for high-SOFA patients across all six RL algorithms.

Among all methods, DDQN-Attention achieves the highest survival rate on high-SOFA patients at 90.5\% (190 episodes), demonstrating that attention mechanisms can provide clinically significant benefits for complex, severe cases where dynamic feature weighting is critical. This represents a 1.9--6.2 percentage point improvement over offline RL methods. SAC achieves 88.7\% survival (195 episodes), matching the performance of offline BC (88.6\%, 211 episodes) and CQL (88.5\%, 191 episodes). DDQN-Residual achieves 87.0\% survival (200 episodes), while offline DQN significantly underperforms at 84.3\% survival (185 episodes)—a 6.2 percentage point gap compared to DDQN-Attention and 4.2--4.3 points below BC/CQL.

The high-SOFA analysis reveals nuanced trade-offs between offline and online RL paradigms. DDQN-Attention's superior performance (90.5\%) suggests that online RL with architectural innovations can improve outcomes for the most critical patients. However, the performance gap is relatively modest (1.9 percentage points vs. BC/CQL), and offline methods achieve competitive survival rates (88.5--88.6\%) comparable to SAC (88.7\%). Notably, offline BC and CQL both substantially outperform offline DQN (88.6\% and 88.5\% vs. 84.3\%), indicating that algorithm selection within the offline paradigm is as important as the offline-vs-online distinction. As we will demonstrate in Section~\ref{sec:results:leg}, CQL combines this robust high-SOFA performance with superior interpretability, making it particularly suitable for clinical deployment when environment interaction during training is infeasible. The bottom-left panel of Figure \ref{fig:algorithm-comparison} visualizes these stratified survival rates, highlighting the divergence in high-SOFA performance across algorithms.

\begin{table}[htbp]
\centering
\caption{Performance on high-severity patients (SOFA $\geq$ 11). DDQN-Attention achieves the highest survival rate (90.5\%) on high-SOFA patients, demonstrating the benefit of attention mechanisms for complex cases. Offline RL methods (BC, CQL) achieve competitive survival rates (88.5--88.6\%) comparable to SAC (88.7\%), while offline DQN underperforms (84.3\%).}
\label{tab:sofa-stratified}
\begin{tabular}{lcccc}
\toprule
\multicolumn{5}{c}{\textbf{High SOFA ($\geq$ 11) - Most Severe Patients}} \\
\midrule
\textbf{Model} & \textbf{$n$} & \textbf{Survival (\%)} & \textbf{Avg Return} & \textbf{Avg Length} \\
\midrule
\multicolumn{5}{l}{\textit{Offline RL}} \\
BC              & 211 & 88.6 & 11.63 $\pm$ 9.82  & 8.3 $\pm$ 1.1 \\
CQL             & 191 & 88.5 & 11.55 $\pm$ 9.95  & 8.3 $\pm$ 1.1 \\
DQN             & 185 & 84.3 & 10.29 $\pm$ 11.46 & 8.5 $\pm$ 1.2 \\
\midrule
\multicolumn{5}{l}{\textit{Online RL}} \\
DDQN-Attention  & 190 & \textbf{90.5} & 12.16 $\pm$ 8.79  & 8.0 $\pm$ 1.1 \\
DDQN-Residual   & 200 & 87.0 & 11.10 $\pm$ 10.09 & 8.3 $\pm$ 1.2 \\
SAC             & 195 & 88.7 & 11.62 $\pm$ 9.49  & 8.1 $\pm$ 1.1 \\
\bottomrule
\end{tabular}
\end{table}


% ============================================================
\subsection{LEG Interpretability Analysis}\label{sec:results:leg}

We now present the core contribution of this work: a systematic comparison of interpretability across BC, CQL, and DQN using Linearly Estimated Gradients (LEG) analysis. We analyzed 10 representative states per algorithm, sampled uniformly across SOFA severity levels, and computed feature importance (saliency) scores for the action selected by each policy. The results reveal dramatic and unexpected differences in interpretability magnitude—up to 600-fold—with profound implications for clinical deployment.

\subsubsection{Feature Importance Magnitude Comparison}

Table \ref{tab:interpretability-metrics} summarizes the LEG interpretability metrics for the three RL algorithms. The most striking finding is the \textit{maximum saliency magnitude}, which quantifies the strength of the strongest feature importance signal. CQL achieves a maximum saliency of 40.06 (for systolic blood pressure), indicating a strong gradient: a unit increase in SysBP would decrease the Q-value by approximately 40 units, substantially reducing the likelihood of aggressive treatment. In contrast, BC achieves a maximum saliency magnitude of only 0.78, roughly 50-fold weaker than CQL. DQN exhibits the weakest interpretability signal at 0.069—a \textbf{600-fold difference} compared to CQL (40.06 / 0.069 $\approx$ 580).

This 600-fold difference is not merely a quantitative artifact but reflects fundamental differences in how these algorithms encode decision rules. CQL's strong saliency scores indicate that the policy relies heavily on a small number of clinically relevant features (blood pressure, lactate) with clear decision thresholds—essentially learning an interpretable, threshold-based rule structure similar to clinical guidelines. BC's mixed interpretability (0.05 to 0.78 across states) suggests that it sometimes captures meaningful feature importance but often produces "flat" saliency patterns where all features appear equally (un)important, likely due to overfitting to the behavioral policy's distribution. DQN's uniformly weak saliency (max 0.069) indicates that it has learned a highly non-linear representation where no single feature dominates decision-making; instead, actions depend on complex interactions across many features, making the policy opaque to linear approximations like LEG.

The saliency range (difference between maximum and minimum saliency) further confirms these patterns: CQL exhibits ranges of $\pm$4 to $\pm$40, BC ranges from $\pm$0.05 to $\pm$0.78, and DQN ranges from $\pm$0.02 to $\pm$0.07. Larger ranges indicate clearer differentiation between important and unimportant features. Clinical coherence assessment—whether top-ranked features align with medical knowledge—rates CQL as "excellent" (blood pressure and lactate consistently top-ranked), BC as "mixed" (interpretable in some states, flat in others), and DQN as "poor" (no clear clinical patterns).

\begin{figure}[htbp]
\centering
\includegraphics[width=\textwidth]{../results/figures/leg_interpretability_comparison.png}
\caption{LEG Interpretability Analysis: Performance-Interpretability Trade-off in Sepsis RL. \textbf{Top:} Maximum LEG saliency scores on logarithmic scale reveal dramatic differences: CQL achieves 40.06 (clinically interpretable), BC achieves 0.78 (mixed patterns), and DQN achieves only 0.069 (non-interpretable)—a 600-fold difference between CQL and DQN. \textbf{Middle panels:} Representative feature importance patterns for each algorithm. CQL shows strong negative saliency for clinically relevant features (SysBP, LACTATE, MeanBP), aligning perfectly with Surviving Sepsis Campaign guidelines. BC exhibits state-dependent patterns with moderate saliency for qSOFA and glucose in some states but flat patterns in others. DQN displays uniformly weak, non-interpretable signals across all features. \textbf{Bottom right table:} Summary of interpretability ratings and clinical deployment suitability. \textbf{Key Finding:} CQL achieves comparable survival rates (88.5\% high-SOFA) with 600-fold stronger feature importance signals, demonstrating that the performance-interpretability trade-off is not inevitable.}
\label{fig:leg-comparison}
\end{figure}

\begin{table}[htbp]
\centering
\caption{LEG interpretability metrics comparing three offline RL algorithms across 10 representative states each. Maximum saliency magnitude measures the strength of the strongest feature importance signal. CQL demonstrates 600-fold stronger feature importance signals compared to DQN (40.06 vs. 0.069), with excellent clinical coherence.}
\label{tab:interpretability-metrics}
\begin{tabular}{lcccc}
\toprule
\textbf{Algorithm} & \textbf{Max Saliency} & \textbf{Typical Range} & \textbf{Interpretability} & \textbf{Clinical} \\
                   &                       &                        & \textbf{Rating}           & \textbf{Deployment} \\
\midrule
CQL                & 40.06                 & $\pm$4 to $\pm$40      & Excellent                 & Suitable \\
BC                 & 0.78                  & $\pm$0.05 to $\pm$0.78 & Mixed                     & Requires validation \\
DQN                & 0.069                 & $\pm$0.02 to $\pm$0.07 & Poor                      & Not suitable \\
\bottomrule
\end{tabular}
\end{table}


\subsubsection{Algorithm-Specific Interpretability Patterns}

We now examine the detailed interpretability patterns for each algorithm, revealing the mechanisms underlying the quantitative differences.

\textbf{Conservative Q-Learning (CQL): Strong, Clinically Coherent Patterns.} CQL consistently produces strong, interpretable saliency patterns across all 10 analyzed states. The top-ranked features are invariably physiological variables central to sepsis pathophysiology: systolic blood pressure (SysBP, saliency -40.06), lactate (saliency -37.75), and mean arterial blood pressure (MeanBP, saliency -24.50). The negative saliency scores have a clear clinical interpretation: \textit{decreasing} blood pressure or \textit{increasing} lactate levels drive the policy toward more aggressive treatment (higher IV fluid and vasopressor dosing). This aligns perfectly with Surviving Sepsis Campaign guidelines \citep{rhodes2017ssc}, which recommend fluid resuscitation and vasopressor support for hypotension and hyperlactatemia.

Across all 10 states, CQL exhibits saliency magnitudes exceeding 4.0 for at least one feature, with 8 out of 10 states showing maximum saliencies above 20.0. This consistency indicates that CQL has learned a robust, generalizable decision rule rather than memorizing state-specific actions. The feature hierarchy is also clinically plausible: after blood pressure and lactate, the next most important features are respiratory rate, SpO$_2$, and SOFA score—all established markers of sepsis severity. Notably, less relevant features (e.g., demographic variables like age, race) receive near-zero saliency scores, demonstrating that CQL correctly identifies clinically relevant signals.

The strong interpretability of CQL appears to stem from its conservative value estimation. By penalizing Q-values for out-of-distribution actions, CQL stays close to the behavioral policy's support, learning a Q-function that approximates the heuristic policy's threshold-based logic. Since the heuristic policy itself uses linear decision rules (e.g., "if SysBP < 100, then escalate IV fluids"), CQL's learned Q-function naturally exhibits strong linear gradients with respect to these features. This suggests that conservatism in offline RL not only improves performance robustness but also enhances interpretability—a novel and important finding.

\textbf{Behavior Cloning (BC): Mixed, State-Dependent Interpretability.} BC exhibits highly variable interpretability across the 10 analyzed states. In some states (e.g., State 5), BC produces moderately interpretable patterns with SysBP (-0.78) and qSOFA (0.25) as top features, suggesting the policy has captured the heuristic's emphasis on blood pressure. However, in other states (e.g., States 1 and 7), BC produces nearly "flat" saliency patterns where all features have saliency scores near zero (range: -0.05 to +0.05). These flat patterns indicate that the policy's action selection is insensitive to feature perturbations in those states, likely because BC has memorized a fixed action distribution from the training data without learning the underlying causal relationships.

This state-dependence likely reflects BC's fundamental limitation: it learns $\pi(a|s)$ by matching the behavioral policy's action probabilities but does not distinguish between high-value and low-value actions. In states where the behavioral policy exhibits high certainty (i.e., one action has very high probability), BC can produce interpretable patterns because the strong action preference creates implicit feature importance. However, in states where the behavioral policy is more uncertain or stochastic, BC's learned distribution flattens, and LEG analysis fails to extract meaningful gradients. This inconsistency makes BC unsuitable for clinical deployment without extensive state-by-state validation.

\textbf{Deep Q-Network (DQN): Uniformly Weak, Non-Interpretable Patterns.} DQN exhibits uniformly weak saliency patterns across all 10 analyzed states, with maximum absolute saliency values never exceeding 0.07. The top-ranked features vary arbitrarily across states—INR (0.069) in one state, bilirubin (-0.061) in another, lactate (-0.053) in a third—with no consistent clinical pattern. More importantly, the saliency magnitudes are so small that even the "most important" features have negligible influence compared to CQL's strong signals.

This lack of interpretability is unsurprising given DQN's training paradigm and architecture. DQN was trained online using deep neural networks with three hidden layers (256-256-128 neurons), allowing it to learn highly non-linear Q-functions. While this flexibility enables DQN to capture complex state-action relationships and achieve good performance, it also means that action selection depends on intricate interactions among many features rather than simple linear combinations. LEG, which approximates gradients with linear regression, cannot capture these non-linearities and thus produces weak, uninformative saliency scores.

Additionally, DQN's training involved extensive exploration ($\epsilon$-greedy with $\epsilon$ decaying from 1.0 to 0.05), which may have encouraged the network to encode distributed representations where information is spread across many neurons. In contrast to CQL's conservative penalty that biases the Q-function toward interpretable, threshold-like structures, DQN's loss function (standard TD error) has no incentive for interpretability, allowing the network to converge to an opaque representation.

The clinical implication is clear: while DQN achieves reasonable overall survival (94.0\%), its decisions are fundamentally uninterpretable using LEG analysis. Clinicians would not be able to understand why DQN recommends specific treatments, making it unsuitable for regulatory approval or real-world deployment where explainability is required.

\textbf{Summary: Performance-Interpretability Trade-off is Not Inevitable.} The most important finding from this LEG analysis is that CQL achieves \textit{both} strong performance (88.5\% survival on high-SOFA patients, comparable to BC and baselines) \textit{and} exceptional interpretability (600-fold stronger saliency signals than DQN). This demonstrates that the commonly assumed trade-off between performance and interpretability in reinforcement learning is not inevitable. By incorporating conservatism into the learning objective, CQL learns policies that are simultaneously effective and explainable. This finding has profound implications for clinical AI deployment, where interpretability is not merely desirable but often legally and ethically required.

% End of Results section
